# This future import allows us to reference a class in type annotations before it is declared.
from __future__ import annotations
from abc import ABC, abstractmethod
import csv
import _csv
import re
import os
import hashlib
import yaml
import pandas as pd
from pathlib import Path
import datetime
from typing import List, Iterable, Dict, Callable, Optional, Any, Tuple, Union, cast


class DataEntityStore:
    """
    This is a collection-like class for storing, searching, and retreiving
    ReposcannerDataEntity objects for downstream use by analysis objects. Specificially,
    it roughly follows the Repository design pattern, but we're not calling it that
    because the word 'repository' is already overloaded in this context.

    Data loaded in from reposcanner-data and data generated by repository routines
    gets placed into a DataEntityStore, where it can later be accessed by analyses. This
    class is used by the ReposcannerManager, who intercepts data following the completion of tasks.
    """

    def __init__(self) -> None:
        self._storage: List[ReposcannerDataEntity] = []

    def __len__(self) -> int:
        """
        Length operator for DataEntityStore.

        len(store) == len(self._storage)
        """
        return len(self._storage)

    def __contains__(self, entity: ReposcannerDataEntity) -> bool:
        """
        'in' operator for DataEntityStore.

        entity in store == entity in self._storage
        """
        return entity in self._storage

    def read(self) -> Iterable[ReposcannerDataEntity]:
        """
        Provides a generator to iterate over all the
        data held in the store. Usually callers will
        use getByCriteria() instead of read().
        """
        for entity in self._storage:
            yield entity

    def insert(self, entity: ReposcannerDataEntity) -> None:
        """
        Add the ReposcannerDataEntity to the data store.

        entity: A ReposcannerDataEntity object.
        """
        self._storage.append(entity)

    def remove(self, entity: ReposcannerDataEntity) -> None:
        """
        Remove an entity from the data store, if it exists.
        This method will raise a ValueError if there is no such entity.

        entity: A ReposcannerDataEntity object.
        """
        self._storage.remove(entity)

    def getByCriteria(self, criteria: Callable[[ReposcannerDataEntity], bool]) -> List[ReposcannerDataEntity]:
        """
        Return only those data entities that meet the specified criteria.

        criteria: a function f(entity), which returns True
        if the entity meets the criteria and False otherwise.
        For example...
                def f(entity):
                        return entity.getCreator() == 'NameOfRepositoryRoutine'

        will return only the entities that were generated by that repository.
        """
        return list(filter(criteria, self._storage))


class DataEntityFactory:
    def createAnnotatedCSVData(self, filePath: Union[Path, str]) -> AnnotatedCSVData:
        return AnnotatedCSVData(filePath=filePath)

    def createYAMLData(self, filePath: Union[Path, str]) -> YAMLData:
        return YAMLData(filePath=filePath)


class ReposcannerDataEntity(ABC):
    """
    Abstract base class for data objects that are created and/or used
    by Reposcanner and its mining routines and analyses.
    """

    def __init__(self, filePath: Union[Path, str]) -> None:
        """
        filepath: The path to the file where the data will be written (or read from).
        metadataAttributes: metadata associated with the data entity.
        """
        self._metadataAttributes: dict[str, Any] = {}
        self._filePath = Path(filePath)
        self.setReposcannerExecutionID(None)
        self.setDateCreated(None)
        self.setCreator(None)

    def getFilePath(self) -> Path:
        return self._filePath

    def setMetadataAttribute(self, key: str, value: Any) -> None:
        self._metadataAttributes[key] = value

    def getMetadataAttribute(self, key: str) -> Any:
        return self._metadataAttributes[key]

    def getAttributeKeys(self) -> Iterable[str]:
        return self._metadataAttributes.keys()

    def setReposcannerExecutionID(self, executionid: Optional[str]) -> None:
        """
        executionid: A string containing an id that uniquely identifies
        the particular run of the Reposcanner tool that was used to
        generate the data held by this data entity.
        """
        self.setMetadataAttribute("executionid", executionid)

    def getReposcannerExecutionID(self) -> Optional[str]:
        return cast(Optional[str], self.getMetadataAttribute("executionid"))

    def setDateCreated(self, dt: Optional[datetime.date]) -> None:
        """
        dt: A datetime.date object.
        """
        self.setMetadataAttribute("datecreated", dt)

    def getDateCreated(self) -> Optional[datetime.date]:
        return cast(Optional[datetime.date], self.getMetadataAttribute("datecreated"))

    def setCreator(self, creator: Optional[str]) -> None:
        """
        creator: A string indicating what routine, analysis, etc.
        was responsible for creating this data entity.
        """
        self.setMetadataAttribute("creator", creator)

    def getCreator(self) -> Optional[str]:
        return cast(Optional[str], self.getMetadataAttribute("creator"))

    def fileExists(self) -> bool:
        return os.path.exists(self._filePath)

    def getMD5Hash(self) -> str:
        """
        Compute the MD5 checksum for a file for provenance-tracking purposes.
        """

        hash_md5 = hashlib.md5()
        with open(self._filePath, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()

    @abstractmethod
    def validateMetadata(self) -> bool:
        """
        Should hold routines that validate that all necessary
        metadata is provided and/or is accurate
        (e.g. in the case of datatypes). Returns True or False.
        """
        pass

    @abstractmethod
    def readFromFile(self) -> None:
        """
        Load the data in the file. Data will be accessible via
        this object.
        """
        pass

    @abstractmethod
    def writeToFile(self) -> None:
        """
        Write data held by this object to the file.
        """
        pass


class YAMLData(ReposcannerDataEntity):
    """
    This data entity class represents a YAML file, such as those generated
    by reposcanner-data or config files containing runtime parameters for
    Reposcanner's execution.

    These files have no metadata associated with them.
    """

    def __init__(self, filePath: Union[Path, str]) -> None:
        """
        filepath: The path to the file where the data will be written (or read from).
        data: The data loaded from or written to the output file in dictionary form.
        """
        super().__init__(Path(filePath))
        self._data: Dict[str, Any] = {}

    def validateMetadata(self) -> bool:
        return True

    def getData(self) -> Dict[str, Any]:
        return self._data

    def setData(self, data: Dict[str, Any]) -> None:
        self._data = data

    def readFromFile(self) -> None:
        if not os.path.exists(self.getFilePath()):
            raise OSError("Reposcanner couldn't find the YAML file ({path})\
                        Shutting down as a precaution.".format(path=self.getFilePath()))
        with open(self.getFilePath()) as f:
            try:
                contents = yaml.safe_load(f)
            except yaml.YAMLError as exception:
                print("While loading a YAML file ({path}), Reposcanner encountered \
                                an exception via PyYAML.".format(path=self.getFilePath()))
                raise exception
            if contents is None:
                raise OSError("PyYAML tried parsing a file ({path}), but that \
                                result was None, which means it failed to read the contents of \
                                the file.".format(path=self.getFilePath()))
        self._data = contents

    def writeToFile(self) -> None:
        with open(self.getFilePath(), 'w') as outfile:
            yaml.dump(self._data, outfile, default_flow_style=False)


class AnnotatedCSVData(ReposcannerDataEntity):
    """
    This data entity class represents a CSV file annotated with metadata.
    Getters/setters are provided for the metadata that

    The embedded metadata format we use is consistent with W3C guidelines.
    (Model for Tabular Data and Metadata on the Web, section 5.4).
    """

    def __init__(self, filePath: Union[Path, str]) -> None:
        super().__init__(Path(filePath))
        self._records: List[List[str]] = []
        self.setColumnNames([])
        self.setColumnDatatypes([])
        self.setProjectID(None)
        self.setProjectName(None)
        self.setURL(None)

    def setProjectID(self, projectid: Optional[str]) -> None:
        """
        projectid: A string containing the idea for the project ID associated
        with a repository
        """
        self.setMetadataAttribute("projectid", projectid)

    def getProjectID(self) -> Optional[str]:
        return cast(Optional[str], self.getMetadataAttribute("projectid"))

    def setProjectName(self, projectname: Optional[str]) -> None:
        """
        projectname: A string containing the name of the project associated with
        a repository.
        """
        self.setMetadataAttribute("projectname", projectname)

    def getProjectName(self) -> Optional[str]:
        return cast(Optional[str], self.getMetadataAttribute("projectname"))

    def setURL(self, url: Optional[str]) -> None:
        """
        url: A string containing the URL that points to the repository where the
        data in this file was mined.
        """
        self.setMetadataAttribute("url", url)

    def getURL(self) -> Optional[str]:
        return cast(Optional[str], self.getMetadataAttribute("url"))

    def getColumnNames(self) -> List[str]:
        return cast(List[str], self.getMetadataAttribute("names"))

    def setColumnNames(self, names: List[str]) -> None:
        """
        names: A list of strings containing the (in-order)
        names for each of the columns.
        """
        self.setMetadataAttribute("names", names)

    def getColumnDatatypes(self) -> List[str]:
        return cast(List[str], self.getMetadataAttribute("datatypes"))

    def setColumnDatatypes(self, datatypes: List[str]) -> None:
        """
        datatypes: A list of strings describing the data
        types for each of the columns.
        """
        return self.setMetadataAttribute("datatypes", datatypes)

    def addRecord(self, record: List[Any]) -> None:
        """
        record: A list of objects containing the data needed
        to write out a record. Records are guaranteed to be
        written out in the order that they were received.
        """
        self._records.append(record)

    def getRawRecords(self) -> List[List[Any]]:
        """
        Get a list of lists, each containing the data associated
        with the record. This method is provided for testing purposes
        and users should call getRecordsForDicts instead.
        """
        return self._records

    def getDataFrame(self, firstRowContainsHeaders: bool = False) -> pd.DataFrame:
        """
        Returns file data in the form of a pandas DataFrame.

        firstRowContainsHeaders: False if the column names are stored in metadata, True if
        the column names are found in the first row of the records (default False).
        """
        if not firstRowContainsHeaders:
            return pd.DataFrame.from_records(
                self._records, columns=self.getColumnNames())
        else:
            return pd.DataFrame.from_records(
                self._records[1:], columns=self._records[0])

    def getRecordsAsDicts(self) -> List[Dict[str, Any]]:
        """
        Returns a list of dictionaries, one for each record, that maps
        the names of columns to their respective data in the files.
        """
        columnNames = self.getColumnNames()
        recordDicts = []
        for record in self._records:
            recordDict = {}
            for index in range(len(columnNames)):
                recordDict[columnNames[index]] = record[index]
            recordDicts.append(recordDict)
        return recordDicts

    def validateMetadata(self) -> bool:
        hasExecutionID = self.getReposcannerExecutionID() is not None
        hasCreator = self.getCreator() is not None
        hasDateCreated = self.getDateCreated() is not None
        hasProjectID = self.getProjectID() is not None
        hasProjectName = self.getProjectName() is not None
        hasURL = self.getURL() is not None
        hasColumnNames = len(self.getColumnNames()) > 0
        hasColumnDatatypes = len(self.getColumnDatatypes()) > 0

        return hasExecutionID and hasCreator and hasDateCreated \
            and hasProjectID and hasProjectName and hasURL \
            and hasColumnNames and hasColumnDatatypes

    def readFromFile(self) -> None:
        def readMetadataFromFile(text: str) -> Tuple[str, str]:
            try:
                # TODO: This may turn out to be a fragile way of parsing the
                # metadata line if the metadata value has spaces in it.
                # m = re.match("#([a-zA-Z]+)\w+(.*?)",text)
                # return m.group(1),m.group(2)
                splitResult = text.split()
                return splitResult[0][1:], splitResult[1]
            except Exception as e:
                raise ValueError(
                    "Failed to parse metadata in {path}: {text}".format(
                        path=self.getFilePath(), text=text))
        with open(self.getFilePath(), 'r', newline='\n') as f:
            csvreader = csv.reader(f, delimiter=',', quotechar='|')
            currentlyReadingMetadata = True
            for row in csvreader:
                if currentlyReadingMetadata:
                    if len(row) > 0 and row[0][0] == '#':
                        metadataKey, metadataValue = readMetadataFromFile(row[0])
                        if metadataKey == "names" or metadataKey == "datatypes":
                            # TODO: Parse names and datatypes, which are lists delimited
                            # by semicolons.
                            metadataValueSplit = metadataValue.split(sep=';')
                            self.setMetadataAttribute(metadataKey, metadataValueSplit)
                        elif metadataKey == "datecreated":
                            metadataValueDate = datetime.date.fromisoformat(
                                    metadataValue)
                            self.setMetadataAttribute(metadataKey, metadataValueDate)
                        else:
                            self.setMetadataAttribute(metadataKey, metadataValue)
                    else:
                        currentlyReadingMetadata = False
                else:
                    self.addRecord(row)

    def writeToFile(self) -> None:
        def writeMetadataFieldToFile(csvwriter: _csv._writer, key: str, value: str) -> None:
            csvwriter.writerow(["#{key} {value}".format(key=key, value=value)])
        with open(self.getFilePath(), 'w', newline='\n') as f:
            csvwriter = csv.writer(
                f,
                delimiter=',',
                quotechar='|',
                quoting=csv.QUOTE_MINIMAL)

            # Expected Metadata for CSV files.
            executionid = self.getReposcannerExecutionID()
            creator = self.getCreator()
            dateCreated = self.getDateCreated()
            projectID = self.getProjectID()
            projectName = self.getProjectName()
            url = self.getURL()
            names = ";".join(self.getColumnNames())
            datatypes = ";".join(self.getColumnDatatypes())

            if executionid is not None:
                writeMetadataFieldToFile(csvwriter, "executionid", executionid)
            if creator is not None:
                writeMetadataFieldToFile(csvwriter, "creator", creator)
            if dateCreated is not None:
                writeMetadataFieldToFile(csvwriter, "datecreated", dateCreated.isoformat())
            if projectID is not None:
                writeMetadataFieldToFile(csvwriter, "projectid", projectID)
            if projectName is not None:
                writeMetadataFieldToFile(csvwriter, "projectname", projectName)
            if url is not None:
                writeMetadataFieldToFile(csvwriter, "url", url)
            writeMetadataFieldToFile(csvwriter, "names", names)
            writeMetadataFieldToFile(csvwriter, "datatypes", datatypes)

            for record in self._records:
                csvwriter.writerow(record)
